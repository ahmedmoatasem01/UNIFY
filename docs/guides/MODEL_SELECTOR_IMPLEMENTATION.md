# Model Selector Implementation

## Overview

The AI Assistant now includes a **model selector** that allows users to choose between:
- **Unify Model** (default) - Fast, lightweight template-based responses
- **Ollama** - Advanced AI with better natural language understanding

---

## Features

### âœ… User Choice
- Users can switch between models at any time
- Model preference is saved in browser localStorage
- Selected model persists across sessions

### âœ… Visual Indicator
- Clear UI showing which model is active
- Tooltips explaining each model's characteristics
- Smooth transitions when switching

### âœ… Backend Integration
- Backend respects user's model choice
- Automatic fallback if Ollama is unavailable
- Proper logging for debugging

---

## Implementation Details

### **1. Frontend (HTML)**

**Location:** `src/templates/ai_assistant.html`

Added model selector in chat header:
```html
<div class="model-selector-container">
    <label class="model-selector-label">
        <i class="fas fa-brain"></i>
        Model:
    </label>
    <div class="model-selector-toggle">
        <button class="model-option active" data-model="unify" id="unifyModelBtn">
            <i class="fas fa-bolt"></i>
            <span>Unify Model</span>
        </button>
        <button class="model-option" data-model="ollama" id="ollamaModelBtn">
            <i class="fas fa-robot"></i>
            <span>Ollama</span>
        </button>
    </div>
</div>
```

---

### **2. Frontend (JavaScript)**

**Location:** `src/static/scripts/ai_assistant.js`

**Changes:**
- Added `selectedModel` property (defaults to 'unify')
- Loads preference from localStorage on initialization
- `selectModel(model)` function to switch models
- `updateModelSelectorUI()` to update visual state
- `sendMessage()` now includes `model` in request body

**Key Functions:**
```javascript
selectModel(model) {
    this.selectedModel = model;
    localStorage.setItem('ai_model_preference', model);
    this.updateModelSelectorUI();
    this.showNotification(`Switched to ${modelName}`, 'success');
}
```

---

### **3. Backend (Controller)**

**Location:** `src/controllers/ai_assistant_controller.py`

**Changes:**
- Reads `model` from request JSON
- Only uses Ollama if:
  - User selected 'ollama' AND
  - Ollama service is available
- Otherwise uses Unify Model (template-based)

**Key Logic:**
```python
selected_model = data.get('model', 'unify')  # Default to 'unify'

# Use LLM service only if user selected 'ollama' and service is available
llm_service_to_use = None
if selected_model == 'ollama' and llm_service:
    llm_service_to_use = llm_service
else:
    # Use Unify Model (template-based)
    llm_service_to_use = None
```

---

### **4. Service Layer**

**Location:** `src/services/ai_assistant_service.py`

**Changes:**
- Updated log messages to use "Unify Model" terminology
- Updated fallback messages to reference "Unify Model"

**Before:**
```python
print(f"[RAG Engine] Using LLM ({llm_service.provider})...")
print(f"[RAG Engine] LLM error, falling back to template...")
```

**After:**
```python
print(f"[RAG Engine] Using Ollama ({llm_service.provider})...")
print(f"[RAG Engine] Ollama error, falling back to Unify Model...")
print(f"[RAG Engine] Using Unify Model (template-based)...")
```

---

### **5. Styling (CSS)**

**Location:** `src/static/styles/ai_assistant.css`

**Added Styles:**
- `.model-selector-container` - Container for selector
- `.model-selector-label` - Label styling
- `.model-selector-toggle` - Toggle button container
- `.model-option` - Individual model button
- `.model-option.active` - Active state styling

**Features:**
- Smooth transitions
- Hover effects
- Active state highlighting
- Responsive design

---

## User Experience

### **Default Behavior**
- On first visit: **Unify Model** is selected (fast, lightweight)
- Preference saved: Next visit remembers last choice

### **Switching Models**
1. Click on desired model button
2. Visual feedback: Button highlights
3. Notification: "Switched to [Model Name]"
4. Next question uses selected model

### **Model Characteristics**

#### **Unify Model** âš¡
- **Speed:** <500ms response time
- **Resources:** ~50MB RAM
- **Quality:** Good for simple questions
- **Best for:** Quick lookups, direct answers

#### **Ollama** ðŸ¤–
- **Speed:** 2-5 seconds response time
- **Resources:** 4-8GB RAM
- **Quality:** Excellent for complex questions
- **Best for:** Natural conversations, synthesis

---

## Technical Flow

### **Request Flow:**
```
1. User selects model in UI
   â†“
2. JavaScript stores preference in localStorage
   â†“
3. User asks question
   â†“
4. JavaScript sends: { question, model: 'unify' | 'ollama' }
   â†“
5. Backend receives request
   â†“
6. Backend checks: selected_model == 'ollama' && llm_service available?
   â†“
7a. YES â†’ Use Ollama LLM service
7b. NO â†’ Use Unify Model (template-based)
   â†“
8. Generate answer
   â†“
9. Return response to frontend
```

---

## Error Handling

### **Ollama Unavailable**
- If user selects Ollama but service is unavailable:
  - Backend automatically falls back to Unify Model
  - User still gets an answer (just using Unify Model)
  - No error shown to user (seamless fallback)

### **Ollama Error**
- If Ollama service fails during generation:
  - Service layer catches error
  - Falls back to Unify Model
  - Logs error for debugging
  - User gets answer from Unify Model

---

## Configuration

### **Default Model**
- Default: `'unify'` (Unify Model)
- Can be changed in JavaScript: `this.selectedModel = 'unify'`

### **Storage**
- Uses browser `localStorage`
- Key: `'ai_model_preference'`
- Value: `'unify'` or `'ollama'`

### **Backend**
- No configuration needed
- Automatically detects if Ollama is available
- Respects user's choice

---

## Testing

### **Test Cases:**
1. âœ… Default model is Unify Model
2. âœ… Switching to Ollama updates UI
3. âœ… Switching to Unify Model updates UI
4. âœ… Preference persists after page reload
5. âœ… Request includes correct model parameter
6. âœ… Backend uses correct model based on choice
7. âœ… Fallback works if Ollama unavailable
8. âœ… CSS styling looks correct

---

## Future Enhancements

### **Potential Improvements:**
- Show model indicator in chat messages
- Display response time for each model
- Add model-specific settings (temperature, etc.)
- Show model availability status
- Add keyboard shortcuts for switching

---

## Files Modified

1. âœ… `src/templates/ai_assistant.html` - Added model selector UI
2. âœ… `src/static/scripts/ai_assistant.js` - Added model selection logic
3. âœ… `src/controllers/ai_assistant_controller.py` - Respects model choice
4. âœ… `src/services/ai_assistant_service.py` - Updated terminology
5. âœ… `src/static/styles/ai_assistant.css` - Added model selector styles

---

## Summary

âœ… **Model selector fully implemented**
âœ… **Users can choose between Unify Model and Ollama**
âœ… **Preference persists across sessions**
âœ… **Automatic fallback if Ollama unavailable**
âœ… **Clean UI with smooth transitions**
âœ… **Proper terminology ("Unify Model" instead of "template-based")**

**The feature is ready to use!** ðŸŽ‰
